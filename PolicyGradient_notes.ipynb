{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PolicyGradient_notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KobryLee/ML-2021Spring-NTU-hws/blob/main/PolicyGradient_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 2\n",
        "1. This assignment is due in two weeks, at 23:59 Feb 11th 2022.\n",
        "2. There are two files to submit. Please name your .py and .ipynb file using your student number as Axxxxxx.py, Axxxxxxx.ipynb and submit it to Luminus->assignments->submissions->assignment2"
      ],
      "metadata": {
        "id": "g9iqbJnc_D1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Policy Gradients\n",
        "\n",
        "You will implement the vanilla policy gradients algorithm, also referred to as\n",
        "REINFORCE.\n",
        "\n",
        "## Review\n",
        "\n",
        "In policy gradients, the objective is to learn a parameter $\\theta^*$ that\n",
        "maximizes the following objective:\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta(\\tau)}[R(\\tau)]\n",
        "\\end{equation}\n",
        "\n",
        "where $\\tau = (s_1,a_1,s_2,\\ldots,s_{T-1},a_{T-1},s_T)$ is a *trajectory*\n",
        "(also referred to as an *episode*), and factorizes as\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi_\\theta(\\tau) = p(s_1)\\pi_\\theta(a_1|s_1)\\prod_{t=2}^{T} p(s_t|s_{t-1},a_{t-1})\\pi_\\theta(a_t|s_t)\n",
        "\\end{equation}\n",
        "\n",
        "and $R(\\tau)$ denotes the full trajectory reward $R(\\tau) = \\sum_{t=1}^{T}\n",
        "r(s_t,a_t)$ with $r(s_t,a_t)$ the rewards at the individual time steps.\n",
        "\n",
        "In policy gradients, we directly apply the gradient $\\nabla_\\theta$ to\n",
        "$J(\\theta)$. In order to do so, we require samples of trajectories, meaning that\n",
        "we now denote them as $\\tau_i$ for the $i$th trajectory, and have $\\tau_i =\n",
        "(s_{i1},a_{i1},s_{i2},\\ldots,s_{iT})$. When we approximate the gradient with\n",
        "samples, we get:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_\\theta J(\\theta) &\\approx \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\log \\pi_\\theta(\\tau_i) R(\\tau_i) \\\\\n",
        "&= \\frac{1}{N}\\sum_{i=1}^N \\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right)  \\left( \\sum_{t=1}^{T} r(s_{it},a_{it}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "Multiplying a discount factor $\\gamma$ to the rewards can be interpreted as\n",
        "encouraging the agent to focus on rewards closer in the future, which can also\n",
        "be thought of as a means for reducing variance (because there are more\n",
        "possible futures further into the future). The discount factor can be\n",
        "incorporated in two ways, from the full trajectory:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "and from the reward to go:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t'=t}^T \\gamma^{t'-t} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "**In this assignment, we only focus on the first version: full tragectory.**\n",
        "\n"
      ],
      "metadata": {
        "id": "lCWlbWAwiOx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradients Implementation\n",
        "\n",
        "\n",
        "**You will need to write code in `PolicyGradient.ipynb`. The places where you need to write code are\n",
        " clearly indicated with the comments `START OF YOUR CODE` and\n",
        "`END OF YOUR CODE`. \n",
        "You do not need to change any other files for this part of the assignment.**\n",
        "\n",
        "The dataflow of the code is structured like this: \n",
        "\n",
        "- Set Up Hyperparameters and environment.\n",
        "- Build a MLP model for policy learning.\n",
        "- Initialize the agent, such as define the policy network and optimizer.\n",
        "- Forward Computation: Sample trajectories by conducting an action given an observation from the environment, and calculate sum of rewards in each trajectory, That includes `sample_action`, `sample_trajectory`, `sample_trajectories` and `sum_of_rewards`.\n",
        "- Backward Computation: Optimize the policy network based on the update rule. That contains `compute_advantage`, `estimate_return`, `get_log_prob` , `update_parameters`.\n",
        "\n",
        "## Problem 1: data sampling\n",
        "\n",
        "You need to implement any parts with a \"Problem 1\" header in the code. Here's what you need to do:\n",
        "\n",
        "- 1. Implement `sample_action`, which samples an action from $\\pi_\\theta(a|s)$. This operation will be called in `sample_trajectories`.\n",
        "- 2. Implement `sample_trajectory`, you need to call `sample_action` to obtain current action.\n",
        "- 3. Implement `sum_of_rewards`, which is the Monte Carlo estimation of the Q function. You need to estimate the q-value of each path and return a single vector for the estimated q values whose length is the sum of the lengths of the paths.\n",
        "\n",
        "## Problem 2: apply policy gradient\n",
        "You only need to implement the parts with the \"Problem 2\" header.\n",
        "\n",
        "- **Estimate return**: in `estimate_return`, normalize the advantages to have a mean of zero and a standard deviation of one.  This is a trick for reducing variance.\n",
        "- Implement `get_log_prob` to obtain $\\log \\pi_\\theta(a_{it}|s_{it})$: Given an action that the agent took in the environment, this computes the log probability of that action under $\\pi_\\theta(a|s)$. This will be used in the parameters update: \n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "- **Update parameters**: In `update_parameters`, using the update operation `optimizer.step()` to update the parameters of the policy. You firstly need to create loss value with the inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "7UzZoFBEnaN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Introduction: \n",
        "\n",
        "\n",
        "##[CartPole-v0](https://gym.openai.com/envs/CartPole-v0/): \n",
        "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077). A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
        "\n",
        "### Observation Space\n",
        "The observation is a `ndarray` with shape `(4,)` where the elements correspond to the following:\n",
        "\n",
        "| Num | Observation           | Min                  | Max                |\n",
        "|-----|-----------------------|----------------------|--------------------|\n",
        "| 0   | Cart Position         | -4.8*                |  4.8*                |\n",
        "| 1   | Cart Velocity         | -Inf                 | Inf                |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°)** | ~ 0.418 rad (24°)** |\n",
        "| 3   | Pole Angular Velocity | -Inf                 | Inf                 |\n",
        "\n",
        "- `*`: the cart x-position can be observed between `(-4.8, 4.8)`, but an episode terminates if the cart leaves the\n",
        "    `(-2.4, 2.4)` range.\n",
        "- `**`: Similarly, the pole angle can be observed between  `(-.418, .418)` radians or precisely **±24°**, but an episode is\n",
        "    terminated if the pole angle is outside the `(-.2095, .2095)` range or precisely **±12°**\n",
        "\n",
        "### Action Space\n",
        "The agent take a 1-element vector for actions.\n",
        "The action space is `(action)` in `[0, 1]`, where `action` is used to push\n",
        "the cart with a fixed amount of force:\n",
        "\n",
        "| Num | Action                 |\n",
        "|-----|------------------------|\n",
        "| 0   | Push cart to the left  |\n",
        "| 1   | Push cart to the right |\n",
        "\n",
        "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing.\n",
        "This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
        "\n",
        "### Rewards\n",
        "Reward is 1 for every step taken, including the termination step.\n",
        "### Starting State\n",
        "All observations are assigned a uniform random value between (-0.05, 0.05).\n",
        "### Episode Termination\n",
        "The episode terminates of one of the following occurs:\n",
        "1. Pole Angle is more than ±12°\n",
        "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
        "3. Episode length is greater than 200. \n"
      ],
      "metadata": {
        "id": "vSvcalq4OcIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.10.5"
      ],
      "metadata": {
        "id": "l1gjw8-xs1r8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f8f89e-4b20-4a4e-8cb5-0d1cd7f0e7c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.10.5\n",
            "  Downloading gym-0.10.5.tar.gz (1.5 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 174 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 235 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 286 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 296 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 307 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 317 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 327 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 348 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 358 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 368 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 378 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 389 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 399 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 450 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 460 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 471 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 481 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 491 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 501 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 512 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 532 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 542 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 552 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 563 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 573 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 583 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 593 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 604 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 614 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 624 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 634 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 645 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 655 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 675 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 686 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 696 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 706 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 716 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 727 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 737 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 747 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 757 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 768 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 778 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 788 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 798 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 819 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 829 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 839 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 849 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 860 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 870 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 880 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 890 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 901 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 911 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 921 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 931 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 942 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 952 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 962 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 972 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 983 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 993 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.0 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.5 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.5 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 24.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym==0.10.5) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.10)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.5-py3-none-any.whl size=1581307 sha256=0c9ec510fa39e3416a4f4f5104fec250433d75c87fcbe867fa4e94726618db7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/2c/df/a05b548a40fae16ca400ecbeda0067e1a296499c1fbd7e0c9a\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qPM8ZfzReLBm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "import time\n",
        "import inspect\n",
        "import sys\n",
        "from multiprocessing import Process\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Hyperparameters"
      ],
      "metadata": {
        "id": "XMeurTX_Qnac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'CartPole-v0'\n",
        "# exp_name = 'vpg'\n",
        "render = False\n",
        "animate = render\n",
        "discount = 1.0\n",
        "n_iter = 101\n",
        "batch_size = 1000\n",
        "ep_len = -1.\n",
        "learning_rate = 5e-3\n",
        "reward_to_go = False\n",
        "dont_normalize_advantages = False\n",
        "seed = 1\n",
        "n_experiments = 1\n",
        "max_path_length = ep_len if ep_len > 0 else None\n",
        "min_timesteps_per_batch = batch_size\n",
        "gamma = discount\n",
        "normalize_advantages = not(dont_normalize_advantages)"
      ],
      "metadata": {
        "id": "RWAgpuL5qG5_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Environment"
      ],
      "metadata": {
        "id": "bjen3xsdQvaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#========================================================================================#\n",
        "# Set Up Env\n",
        "#========================================================================================#\n",
        "\n",
        "# Make the gym environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.seed(seed)\n",
        "\n",
        "# Maximum length for episodes\n",
        "max_path_length = max_path_length or env.spec.max_episode_steps\n",
        "\n",
        "# Is this env continuous, or self.discrete? In this assignment, we only consider discrete action space.\n",
        "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "# Observation and action sizes\n",
        "ob_dim = env.observation_space.shape[0]\n",
        "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
        "# print(ac_dim)"
      ],
      "metadata": {
        "id": "Q86wu9Q1JP_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9a8909-03aa-4380-aa48-7c10aa41d3d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a MLP model for policy learning."
      ],
      "metadata": {
        "id": "mMnEZgWfSiR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, num_actions):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_size, 32)\n",
        "        self.dense2 = nn.Linear(32, 32)\n",
        "        self.dense3 = nn.Linear(32, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.dense1(x))\n",
        "        x = F.tanh(self.dense2(x))\n",
        "        out = F.softmax(self.dense3(x))\n",
        "        return out"
      ],
      "metadata": {
        "id": "FNQgI6V7erIN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Initialize Agent\n",
        "    "
      ],
      "metadata": {
        "id": "Ln23veEJLnRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = MLP(input_size=ob_dim, num_actions=ac_dim)\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "_IKxBHwBFnDc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sampling"
      ],
      "metadata": {
        "id": "bFVJfx6z-RgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_action(policy_parameters):\n",
        "    \"\"\"\n",
        "    Stochastically sampling from the policy distribution\n",
        "\n",
        "    arguments:\n",
        "        policy_parameters: logits of a categorical distribution over actions\n",
        "                sy_logits_na: (batch_size, self.ac_dim)\n",
        "\n",
        "    returns:\n",
        "        sy_sampled_ac: (batch_size,)\n",
        "    \"\"\"\n",
        "\n",
        "    sy_logits_na = policy_parameters\n",
        "    #========================================================================================#\n",
        "    #                           ----------PROBLEM 1----------\n",
        "    #========================================================================================#\n",
        "    # Stochastically sampling an action from the policy distribution $\\pi_\\theta(a|s)$.\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    #                   time 0              time 1\n",
        "    # sample_input = [[0.1,0.2,0.3,0.4],[0.4,0.3,0.2,0.1]]\n",
        "    # sample_output = [3,0]\n",
        "    # 0.16 prob\n",
        "    # log(0.4) + log(0.4)\n",
        "    # torch has implemented api for it\n",
        "    # ------------------------------------------------------------------\n",
        "    sy_sampled_ac = torch.multinomial(sy_logits_na,1).reshape(-1)\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    return sy_sampled_ac"
      ],
      "metadata": {
        "id": "ctKFEkatGfDH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = [[0.1,0.2,0.3,0.4],[0.4,0.3,0.2,0.1]]\n",
        "for i in range(10):\n",
        "  print(sample_action(torch.tensor(sample_input)))"
      ],
      "metadata": {
        "id": "F3oEEgXWvaaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4498e0-2ff3-449d-ce86-a520d49d9504"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 0])\n",
            "tensor([2, 1])\n",
            "tensor([3, 1])\n",
            "tensor([1, 1])\n",
            "tensor([3, 0])\n",
            "tensor([3, 2])\n",
            "tensor([3, 2])\n",
            "tensor([2, 1])\n",
            "tensor([0, 0])\n",
            "tensor([1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_trajectory(env):\n",
        "    ob = env.reset()\n",
        "    obs, acs, rewards = [], [], []\n",
        "    steps = 0\n",
        "    while True:\n",
        "\n",
        "        obs.append(ob)\n",
        "        #====================================================================================#\n",
        "        #                           ----------PROBLEM 1----------\n",
        "        #====================================================================================#\n",
        "        # obtain the action 'ac' for current observation 'ob'\n",
        "        # ------------------------------------------------------------------\n",
        "        # START OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "        logits=policy_net(torch.tensor(obs).to(torch.float32))\n",
        "        ac=sample_action(logits)\n",
        "\n",
        "\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # END OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "        ac = ac.numpy()[0]\n",
        "        acs.append(ac)\n",
        "        ob, rew, done, _ = env.step(ac)\n",
        "        rewards.append(rew)\n",
        "        steps += 1\n",
        "        if done or steps > max_path_length:\n",
        "            break\n",
        "    path = {\"observation\" : np.array(obs, dtype=np.float32),\n",
        "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
        "            \"action\" : np.array(acs, dtype=np.float32)}\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "Z7hSoAK0HP81"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_trajectories(itr, env):\n",
        "    \"\"\"Collect paths until we have enough timesteps, as determined by the\n",
        "    length of all paths collected in this batch.\n",
        "    \"\"\"\n",
        "    timesteps_this_batch = 0\n",
        "    paths = []\n",
        "    while True:\n",
        "        path = sample_trajectory(env)\n",
        "        paths.append(path)\n",
        "        timesteps_this_batch += len(path[\"reward\"])\n",
        "        if timesteps_this_batch > min_timesteps_per_batch:\n",
        "            break\n",
        "    return paths, timesteps_this_batch"
      ],
      "metadata": {
        "id": "UdwFJkpbHGmt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For sum of rewards, we use the total discounted reward summed over entire trajectory (regardless of which time step the Q-value should be for)."
      ],
      "metadata": {
        "id": "-c4Rgb7jC9jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_of_rewards(re_n):\n",
        "    \"\"\" Monte Carlo estimation of the Q function.\n",
        "\n",
        "    let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
        "        the function sample_trajectories\n",
        "    let num_paths be the number of paths sampled from sample_trajectories\n",
        "\n",
        "    arguments:\n",
        "        re_n: length: num_paths. Each element in re_n is a numpy array\n",
        "            containing the rewards for the particular path\n",
        "            [[1,1,1],[1,1,1,1]]\n",
        "\n",
        "    returns:\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "            [3,3,3,4,4,4,4]\n",
        "            [3,2,1,4,3,2,1] (to be test)\n",
        "            0,1,0\n",
        "            0,0,0,1\n",
        "    ----------------------------------------------------------------------------------\n",
        "\n",
        "    Your code should construct numpy arrays for Q-values which will be used to compute\n",
        "    advantages.\n",
        "\n",
        "\n",
        "    You will write code for trajectory-based PG: \n",
        "\n",
        "          We use the total discounted reward summed over\n",
        "          entire trajectory (regardless of which time step the Q-value should be for).\n",
        "\n",
        "          For this case, the policy gradient estimator is\n",
        "\n",
        "              E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * Ret(tau)]\n",
        "\n",
        "          where\n",
        "\n",
        "              tau=(s_0, a_0, ...) is a trajectory,\n",
        "              Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}.\n",
        "\n",
        "          Thus, you should compute\n",
        "\n",
        "              Q_t = Ret(tau)\n",
        "\n",
        "    Store the Q-values for all timesteps and all trajectories in a variable 'q_n',\n",
        "    like the 'ob_no' and 'ac_na' above.\n",
        "    \"\"\"\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 1----------\n",
        "    #====================================================================================#\n",
        "    # q_n: A single vector for the estimated q values whose length is the sum of the lengths of the paths.\n",
        "    # Q-values: Q_t = Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}. \n",
        "    # Store the Q-values for all timesteps and all trajectories in a variable 'q_n'.\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    q_n=[]\n",
        "    for i in re_n:\n",
        "      temp=sum(i)\n",
        "      for j in i:\n",
        "        q_n.append(temp)\n",
        "\n",
        "    # # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    return q_n"
      ],
      "metadata": {
        "id": "7HFdZ45SHm-g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[[1,2,3],[2,3,4,5]]\n",
        "print(sum(a[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egFuLqMoFhWu",
        "outputId": "3df9ee2a-4f06-4fd8-e3ff-f308617196a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Policy Gradient\n",
        "\n",
        "We firstly need to estimate return `estimate_return` and calculate log probability of actions `get_log_prob`. Then we can update parameters based on the rule:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "W86DkWbVgNvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_advantage(ob_no, q_n):\n",
        "  \n",
        "    adv_n = q_n.copy()\n",
        "    return adv_n"
      ],
      "metadata": {
        "id": "3gdKRz2UH6BE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_return(ob_no, re_n):\n",
        "    \"\"\" Estimates the returns over a set of trajectories.\n",
        "\n",
        "    let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
        "        sample_trajectories\n",
        "    let num_paths be the number of paths sampled from sample_trajectories\n",
        "\n",
        "    arguments:\n",
        "        ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
        "        re_n: length: num_paths. Each element in re_n is a numpy array\n",
        "            containing the rewards for the particular path\n",
        "\n",
        "    returns:\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "        adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
        "            advantages whose length is the sum of the lengths of the paths\n",
        "    \"\"\"\n",
        "    q_n = sum_of_rewards(re_n)\n",
        "    adv_n = compute_advantage(ob_no, q_n)\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    # Advantage Normalization\n",
        "    #====================================================================================#\n",
        "    if normalize_advantages:\n",
        "        # On the next line, implement a trick which is known empirically to reduce variance\n",
        "        # in policy gradient methods: normalize adv_n to have mean zero and std=1.\n",
        "        # ------------------------------------------------------------------\n",
        "        # START OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "\n",
        "        # print(type(adv_n))\n",
        "\n",
        "        mu = np.mean(adv_n, axis=0)\n",
        "        sigma = np.std(adv_n, axis=0)\n",
        "        adv_n = (adv_n - mu) / sigma\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # END OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "    return q_n, adv_n"
      ],
      "metadata": {
        "id": "p0bSKK4KICGo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_log_prob(policy_parameters, sy_ac_na):\n",
        "    \"\"\"\n",
        "    Computing the log probability of a set of actions that were actually taken according to the policy\n",
        "\n",
        "    arguments:\n",
        "        policy_parameters: logits of a categorical distribution over actions\n",
        "                sy_logits_na: (batch_size, self.ac_dim)\n",
        "\n",
        "        sy_ac_na: (batch_size,)\n",
        "\n",
        "    returns:\n",
        "        sy_logprob_n: (batch_size)\n",
        "\n",
        "    Hint:\n",
        "        For the discrete case, use the log probability under a categorical distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    sy_logits_na = policy_parameters\n",
        "    #========================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    #========================================================================================#\n",
        "    # sy_logprob_n = \\sum_{t=1}^T \\log \\pi_\\theta(a_{it}|s_{it})\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    for i in sy_ac_na:\n",
        "      \n",
        "\n",
        "    sy_logprob_n=0\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    return sy_logprob_n"
      ],
      "metadata": {
        "id": "mfS5P1B6Gq5B"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(ob_no, ac_na, q_n, adv_n):\n",
        "    \"\"\"\n",
        "    Update the parameters of the policy and (possibly) the neural network baseline,\n",
        "    which is trained to approximate the value function.\n",
        "\n",
        "    arguments:\n",
        "        ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
        "        ac_na: shape: (sum_of_path_lengths).\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "        adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
        "            advantages whose length is the sum of the lengths of the paths\n",
        "    q_n: speed, and final performance of convergence?\n",
        "    q_n-target?\n",
        "    returns:\n",
        "        nothing\n",
        "    \"\"\"\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    #====================================================================================#\n",
        "    # Performing the Policy Update based on the current batch of rollouts.\n",
        "    # \n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    log_prob = get_log_prob(policy_net(torch.tensor(ob_no).to(torch.float32)),ac_na)\n",
        "\n",
        "    loss=-\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "S_xQL3kPINdl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop."
      ],
      "metadata": {
        "id": "4zp8VyVQgQ-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Running experiment with seed %d'%seed)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "total_timesteps = 0\n",
        "\n",
        "return_data = []\n",
        "\n",
        "for itr in range(n_iter):\n",
        "\n",
        "    paths, timesteps_this_batch = sample_trajectories(itr, env)\n",
        "    total_timesteps += timesteps_this_batch\n",
        "\n",
        "    # Build arrays for observation, action for the policy gradient update by\n",
        "    # concatenating across paths\n",
        "    ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
        "    ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
        "\n",
        "    re_n = [path[\"reward\"] for path in paths]\n",
        "\n",
        "    q_n, adv_n = estimate_return(ob_no, re_n)\n",
        "\n",
        "\n",
        "    update_parameters(ob_no, ac_na, q_n, adv_n)\n",
        "\n",
        "    # Log diagnostics\n",
        "    returns = [path[\"reward\"].sum() for path in paths]\n",
        "\n",
        "    if itr%10 == 0:\n",
        "        print(\"********** Iteration %i ************\"%itr)\n",
        "        ep_lengths = [len(path[\"reward\"]) for path in paths]\n",
        "        print(\"Time: \", time.time() - start)\n",
        "        print(\"Iteration: \", itr)\n",
        "        print(\"AverageReturn: \", np.mean(returns))\n",
        "        print(\"StdReturn: \", np.std(returns))\n",
        "        print(\"MaxReturn: \", np.max(returns))\n",
        "        print(\"MinReturn\", np.min(returns))\n",
        "        print(\"EpLenMean: \", np.mean(ep_lengths))\n",
        "        print(\"EpLenStd: \", np.std(ep_lengths))\n",
        "        print(\"TimestepsThisBatch: \", timesteps_this_batch)\n",
        "        print(\"TimestepsSoFar: \", total_timesteps)\n",
        "    return_data.append(np.mean(returns))"
      ],
      "metadata": {
        "id": "NqS80le8pjC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2486f38f-c58d-4bc7-8008-9fc31c723dda"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment with seed 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5023, 0.4977],\n",
            "        [0.5145, 0.4855],\n",
            "        [0.5259, 0.4741],\n",
            "        ...,\n",
            "        [0.4582, 0.5418],\n",
            "        [0.4506, 0.5494],\n",
            "        [0.4598, 0.5402]], grad_fn=<SoftmaxBackward0>)\n",
            "[1. 1. 0. ... 0. 1. 1.]\n",
            "********** Iteration 0 ************\n",
            "Time:  0.27245330810546875\n",
            "Iteration:  0\n",
            "AverageReturn:  22.288889\n",
            "StdReturn:  11.4088125\n",
            "MaxReturn:  55.0\n",
            "MinReturn 10.0\n",
            "EpLenMean:  22.288888888888888\n",
            "EpLenStd:  11.408811842357686\n",
            "TimestepsThisBatch:  1003\n",
            "TimestepsSoFar:  1003\n",
            "tensor([[0.5027, 0.4973],\n",
            "        [0.4905, 0.5095],\n",
            "        [0.4789, 0.5211],\n",
            "        ...,\n",
            "        [0.5608, 0.4392],\n",
            "        [0.5535, 0.4465],\n",
            "        [0.5455, 0.4545]], grad_fn=<SoftmaxBackward0>)\n",
            "[0. 0. 1. ... 0. 0. 0.]\n",
            "tensor([[0.5031, 0.4969],\n",
            "        [0.4909, 0.5091],\n",
            "        [0.4792, 0.5208],\n",
            "        ...,\n",
            "        [0.5280, 0.4720],\n",
            "        [0.5152, 0.4848],\n",
            "        [0.5027, 0.4973]], grad_fn=<SoftmaxBackward0>)\n",
            "[0. 0. 1. ... 0. 0. 1.]\n",
            "tensor([[0.5002, 0.4998],\n",
            "        [0.5123, 0.4877],\n",
            "        [0.5237, 0.4763],\n",
            "        ...,\n",
            "        [0.5011, 0.4989],\n",
            "        [0.5126, 0.4874],\n",
            "        [0.5226, 0.4774]], grad_fn=<SoftmaxBackward0>)\n",
            "[1. 1. 1. ... 1. 1. 0.]\n",
            "tensor([[0.5002, 0.4998],\n",
            "        [0.5123, 0.4877],\n",
            "        [0.5238, 0.4762],\n",
            "        ...,\n",
            "        [0.4677, 0.5323],\n",
            "        [0.4787, 0.5213],\n",
            "        [0.4685, 0.5315]], grad_fn=<SoftmaxBackward0>)\n",
            "[1. 1. 0. ... 1. 0. 0.]\n",
            "tensor([[0.4986, 0.5014],\n",
            "        [0.5107, 0.4893],\n",
            "        [0.5222, 0.4778],\n",
            "        ...,\n",
            "        [0.4778, 0.5222],\n",
            "        [0.4682, 0.5318],\n",
            "        [0.4601, 0.5399]], grad_fn=<SoftmaxBackward0>)\n",
            "[1. 1. 1. ... 0. 0. 1.]\n",
            "tensor([[0.4990, 0.5010],\n",
            "        [0.4868, 0.5132],\n",
            "        [0.4753, 0.5247],\n",
            "        ...,\n",
            "        [0.5313, 0.4687],\n",
            "        [0.5211, 0.4789],\n",
            "        [0.5102, 0.4898]], grad_fn=<SoftmaxBackward0>)\n",
            "[0. 0. 0. ... 0. 0. 1.]\n",
            "tensor([[0.5034, 0.4966],\n",
            "        [0.4911, 0.5089],\n",
            "        [0.4794, 0.5206],\n",
            "        ...,\n",
            "        [0.5233, 0.4767],\n",
            "        [0.5325, 0.4675],\n",
            "        [0.5226, 0.4774]], grad_fn=<SoftmaxBackward0>)\n",
            "[0. 0. 0. ... 1. 0. 0.]\n",
            "tensor([[0.5000, 0.5000],\n",
            "        [0.4879, 0.5121],\n",
            "        [0.4764, 0.5236],\n",
            "        ...,\n",
            "        [0.4574, 0.5426],\n",
            "        [0.4499, 0.5501],\n",
            "        [0.4591, 0.5409]], grad_fn=<SoftmaxBackward0>)\n",
            "[0. 0. 0. ... 0. 1. 1.]\n",
            "tensor([[0.5003, 0.4997],\n",
            "        [0.5124, 0.4876],\n",
            "        [0.5239, 0.4761],\n",
            "        ...,\n",
            "        [0.5406, 0.4594],\n",
            "        [0.5304, 0.4696],\n",
            "        [0.5195, 0.4805]], grad_fn=<SoftmaxBackward0>)\n",
            "[1. 1. 1. ... 0. 0. 1.]\n",
            "tensor([[0.5035, 0.4965],\n",
            "        [0.5157, 0.4843],\n",
            "        [0.5034, 0.4966],\n",
            "        ...,\n",
            "        [0.5575, 0.4425],\n",
            "        [0.5620, 0.4380],\n",
            "        [0.5548, 0.4452]], grad_fn=<SoftmaxBackward0>)\n",
            "[1. 0. 0. ... 1. 0. 1.]\n",
            "********** Iteration 10 ************\n",
            "Time:  2.790468215942383\n",
            "Iteration:  10\n",
            "AverageReturn:  25.525\n",
            "StdReturn:  16.484821\n",
            "MaxReturn:  87.0\n",
            "MinReturn 9.0\n",
            "EpLenMean:  25.525\n",
            "EpLenStd:  16.48482256501416\n",
            "TimestepsThisBatch:  1021\n",
            "TimestepsSoFar:  11127\n",
            "tensor([[0.5037, 0.4963],\n",
            "        [0.4915, 0.5085],\n",
            "        [0.4798, 0.5202],\n",
            "        ...,\n",
            "        [0.4384, 0.5616],\n",
            "        [0.4464, 0.5536],\n",
            "        [0.4410, 0.5590]], grad_fn=<SoftmaxBackward0>)\n",
            "[0. 0. 0. ... 1. 0. 0.]\n",
            "tensor([[0.5012, 0.4988],\n",
            "        [0.5134, 0.4866],\n",
            "        [0.5012, 0.4988],\n",
            "        ...,\n",
            "        [0.5412, 0.4588],\n",
            "        [0.5477, 0.4523],\n",
            "        [0.5526, 0.4474]], grad_fn=<SoftmaxBackward0>)\n",
            "[1. 0. 0. ... 1. 1. 0.]\n",
            "tensor([[0.5014, 0.4986],\n",
            "        [0.4892, 0.5108],\n",
            "        [0.4776, 0.5224],\n",
            "        ...,\n",
            "        [0.5420, 0.4580],\n",
            "        [0.5325, 0.4675],\n",
            "        [0.5222, 0.4778]], grad_fn=<SoftmaxBackward0>)\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "tensor([[0.4988, 0.5012],\n",
            "        [0.4867, 0.5133],\n",
            "        [0.4992, 0.5008],\n",
            "        ...,\n",
            "        [0.5096, 0.4904],\n",
            "        [0.5200, 0.4800],\n",
            "        [0.5288, 0.4712]], grad_fn=<SoftmaxBackward0>)\n",
            "[0. 1. 0. ... 1. 1. 0.]\n",
            "tensor([[0.5004, 0.4996],\n",
            "        [0.5126, 0.4874],\n",
            "        [0.5003, 0.4997],\n",
            "        ...,\n",
            "        [0.4358, 0.5642],\n",
            "        [0.4311, 0.5689],\n",
            "        [0.4276, 0.5724]], grad_fn=<SoftmaxBackward0>)\n",
            "[1. 0. 1. ... 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-54315578c892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps_this_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtimesteps_this_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7a320494c555>\u001b[0m in \u001b[0;36msample_trajectories\u001b[0;34m(itr, env)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtimesteps_this_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0b984e8d0551>\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# ------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-725598234db7>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(policy_parameters)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# torch has implemented api for it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# ------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msy_sampled_ac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msy_logits_na\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Average-Return curve.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D4ifa06fgs7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(return_data)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Return\")"
      ],
      "metadata": {
        "id": "UvnRU2vT0xIj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}